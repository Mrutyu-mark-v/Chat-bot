{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "890e1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import numpy as np \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "import urllib.request\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abfe79e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hell/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/hell/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')   # for WordNet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0735afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "artificial intelligence (ai) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. it is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n",
      "high-profile applications of ai \n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "\n",
    "# Add headers to look like a real browser\n",
    "req = urllib.request.Request(\n",
    "    url,\n",
    "    headers={'User-Agent': 'Mozilla/5.0'}\n",
    ")\n",
    "\n",
    "get_link = urllib.request.urlopen(req).read()\n",
    "\n",
    "data = bs(get_link, 'lxml')\n",
    "data_paragraph = data.find_all('p')\n",
    "\n",
    "data_text = ''\n",
    "for para in data_paragraph:\n",
    "    data_text += para.text\n",
    "\n",
    "data_text = data_text.lower()\n",
    "\n",
    "print(data_text[:500])  # print first 500 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "114cfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = re.sub(r'\\[[0-9]*\\]',' ',data_text)\n",
    "\n",
    "data_text = re.sub(r'\\s+',' ',data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49cc4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokennizetion\n",
    "\n",
    "data_sentence = nltk.sent_tokenize(data_text,language='english')\n",
    "data_word = nltk.word_tokenize(data_text,language='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "449d8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lemetization and preprocess text\n",
    "\n",
    "wnlammatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [wnlammatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "punctuation_removal = dict((ord(punctuation),None) for punctuation in string.punctuation)\n",
    "\n",
    "def get_process_text(document):\n",
    "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5fe00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# greeting function\n",
    "\n",
    "greeting_input = (\"hey\",\"whatsup\",\"good morning\",\"good night\",\"hi\",\"mroning\")\n",
    "\n",
    "greeting_responses =  [\"hey\",\"hey how are you?\",\"*node*\",\"hello, how are you?\",\"hello\",\"wellcom, i am edith\"]\n",
    "\n",
    "def generate_greeting_responses(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_input:\n",
    "            return random.choice(greeting_responses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1920cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating responce\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def generate_response(usre_input):\n",
    "    bot_response = ''\n",
    "    data_sentence.append(usre_input)\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=get_process_text,stop_words='english')\n",
    "    all_word_vector = word_vectorizer.fit_transform(data_sentence)\n",
    "    similar_vector_value = cosine_similarity(all_word_vector[-1],all_word_vector)\n",
    "    similar_sentence_num = similar_vector_value.argsort()[0][-2]\n",
    "\n",
    "    matched_vector = similar_vector_value.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    if vector_matched == 0:\n",
    "        bot_response = bot_response + \" i am sorry i could not understand this.\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response + data_sentence[similar_sentence_num]\n",
    "        return bot_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8960c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, i am from AI BOT, you can ask me any Question regarding AI\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT :hey how are you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what is ai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hell/anaconda3/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/hell/anaconda3/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT : however, many ai applications are not perceived as ai: \"a lot of cutting edge ai has filtered into general applications, often without being called ai because once something becomes useful enough and common enough it's not labeled ai anymore.\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " give history about ai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT : no established unifying theory or paradigm has guided ai research for most of its history.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " history\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT : no established unifying theory or paradigm has guided ai research for most of its history.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " nlp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT : natural language processing (nlp) allows programs to read, write and communicate in human languages.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT :  i am sorry i could not understand this.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " thanks you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT :  i am sorry i could not understand this.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " thank you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT : most welcome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI BOT :hey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what is ai\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI BOT :\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m generate_greeting_responses(human_text))\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;66;03m# print(generate_greeting_responses(human_text))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI BOT :\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m generate_greeting_responses(human_text))\n\u001b[1;32m     19\u001b[0m             data_sentence\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "# bot finish\n",
    "\n",
    "continue_dialog = True\n",
    "print(\"hello, i am from AI BOT, you can ask me any Question regarding AI\")\n",
    "\n",
    "while(continue_dialog==True):\n",
    "    human_text = input()\n",
    "    human_text = human_text.lower()\n",
    "    if human_text != 'bye':\n",
    "        if human_text == 'thanks' or human_text == 'thank you very much' or human_text == 'thank you':\n",
    "            continue_dialog = False\n",
    "            print('AI BOT : most welcome')\n",
    "        else:\n",
    "            if generate_greeting_responses(human_text) is not None:\n",
    "                print('AI BOT :'+ generate_greeting_responses(human_text))\n",
    "            else:\n",
    "                # print(generate_greeting_responses(human_text))\n",
    "                bot_response = generate_response(human_text)\n",
    "                print(\"AI BOT : \" + bot_response)\n",
    "                data_sentence.pop()\n",
    "    else:\n",
    "        continue_dialog = False\n",
    "        print('AI BOT : bye, take of your self ........... ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502fc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
